{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from femsntl.df_verbs import append_max_and_count, case_when, cross_join, find_ids\n",
    "from femsntl.readers import read_file\n",
    "\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Inputs and Outputs\n",
    "\n",
    "In this section we describe the inputs and outputs of this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get timestamp for final output writing\n",
    "## (then use function to read in most current version w/ root filename)\n",
    "current_time = datetime.now().strftime(format=\"%m-%d-%y-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from femsntl.datafiles import INTERMEDIATE_DIR, PRIVATE_DATA_DIR\n",
    "\n",
    "## INPUTS\n",
    "BENEFICIARY_FILE = PRIVATE_DATA_DIR / \"Member_Matches_wDHCF.xlsx\"\n",
    "MEDICARE_FILE = PRIVATE_DATA_DIR / \"MedicareEnrollmentForNTLMembersList.csv\"\n",
    "FILE_SHARED_DHCF = INTERMEDIATE_DIR / \"df_fordhcr_DOBsadded.csv\"\n",
    "CALL_RESPONSE_POST_TX_FILE = INTERMEDIATE_DIR / \"callresponse_forposttx.csv\"\n",
    "CONSTRUCTED_IDS_FILE = INTERMEDIATE_DIR / \"df_forrepeatcalls.csv\"\n",
    "\n",
    "## OUTPUTS\n",
    "MEDICAID_MULTIPLE_NTL_FILE = INTERMEDIATE_DIR / \"medicaid_multiplentl.xlsx\"\n",
    "NTL_BENEFITS_MUTLIPLE_REVIEW_FILE = INTERMEDIATE_DIR / \"ntlbene_mult_toreview.csv\"\n",
    "DECORATED_NTL_OUTPUT_FILE = INTERMEDIATE_DIR / \"ntl_withmedicaidIDS_{}.csv\".format(\n",
    "    current_time\n",
    ")\n",
    "\n",
    "## Hand-edited intermediate inputs\n",
    "MEDICAID_MULTIPLE_NTL_FILE_CODED_R1 = (\n",
    "    INTERMEDIATE_DIR / \"medicaid_multiplentl_RAreview_MR.xlsx\"\n",
    ")\n",
    "\n",
    "MEDICAID_MULTIPLE_NTL_FILE_CODED_R2 = (\n",
    "    INTERMEDIATE_DIR / \"medicaid_multiplentl_RAreview_JG.xlsx\"\n",
    ")\n",
    "\n",
    "NTL_BENEFITS_MUTLIPLE_REVIEW_FILE_CODED = (\n",
    "    INTERMEDIATE_DIR / \"ntlbene_mult_reviewed_by_kevin.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data and merge back with NTL ids\n",
    "\n",
    "In all of what follows, our goal is to answer the question: \"What is going on with multiple matches?\" On the left hand side, there is the Medicaid beneficiary file, which is keyed by `MedicaidSystemID`. On the right hand side, there are the NTL call logs, which are keyed by `date_of_birth` and `FirstLastName1`, which we combine into `name_dob_id`.\n",
    "\n",
    "We examine two types of matches:\n",
    "  1. Cases where a single `name_dob_id` matches to multiple `MedicaidSystemID`s, and\n",
    "  2. Cases where a single `MedicaidSystemID` matches to multiple `name_dob_id`s.\n",
    "\n",
    "In the first case, we send multiple matches back to DHCF and request more information after some manual review.\n",
    "\n",
    "In the second case, we manually review fo\n",
    "\n",
    "Finally, in section 1.3, below, we join these results back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reads beneficiary file that contains all matches\n",
    "beneficiary_file = read_file(BENEFICIARY_FILE)\n",
    "\n",
    "## new: data on medicare enrollment (also have medicaid enrollment spells\n",
    "## but using in next script since those depend more on ntl study dates)\n",
    "medicare_file = read_file(MEDICARE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create indicators of match to help adjudicate\n",
    "## between cases that have multiple matches\n",
    "beneficiary_file[\"matched_fname\"] = (\n",
    "    beneficiary_file.firstname_name1 == beneficiary_file.MemberFirstName\n",
    ")\n",
    "beneficiary_file[\"matched_lname\"] = (\n",
    "    beneficiary_file.lastname_name1 == beneficiary_file.MemberLastName\n",
    ")\n",
    "beneficiary_file[\"matched_dob\"] = (\n",
    "    beneficiary_file.date_of_birth == beneficiary_file.MemberDateofBirth\n",
    ")\n",
    "\n",
    "## match points range from 0-3 and reflect # of fields that\n",
    "beneficiary_file[\"match_points\"] = beneficiary_file[\n",
    "    [\"matched_fname\", \"matched_lname\", \"matched_dob\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "if \"name_dob_id\" not in beneficiary_file.columns:\n",
    "    beneficiary_file[\"name_dob_id\"] = (\n",
    "        beneficiary_file.date_of_birth.astype(str)\n",
    "        + \"_\"\n",
    "        + beneficiary_file.FirstLastName1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare dimensions for beneficiary file\n",
    "print(\n",
    "    f\"There are {len(beneficiary_file)} rows in Medicaid beneficiary file\"\n",
    "    f\"resulting from match, corresponding to {beneficiary_file.name_dob_id.nunique()} unique name-dob pairs\"\n",
    ")\n",
    "\n",
    "## subset data to relevant fields. This is our primary dataframe\n",
    "COMMON_COLS = [\n",
    "    \"MedicaidSystemID\",\n",
    "    \"MemberFullName\",\n",
    "    \"MemberDateofBirth\",\n",
    "    \"match_points\",\n",
    "    \"name_dob_id\",\n",
    "]\n",
    "df = beneficiary_file[COMMON_COLS].drop_duplicates().copy()\n",
    "\n",
    "## find people who within same name and dob, have multiple medicaid ids\n",
    "count_eachname = df.name_dob_id.value_counts()\n",
    "count_eachID = df.MedicaidSystemID.value_counts()\n",
    "\n",
    "## name and dobs of ntlers who appear multiple times\n",
    "ntl_ids_in_many_rows = count_eachname[count_eachname > 1].index\n",
    "\n",
    "## medicaid system ids of medicaid beneficiaries who appear multiple times\n",
    "medicaid_ids_in_many_rows = count_eachID[count_eachID > 1].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Look at case one: One NTL ID, Many Medicaid ids\n",
    "\n",
    "Need to deduplicate because this can't be due to repeat calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_of_both_id_df = df[\n",
    "    ~df[\"name_dob_id\"].isin(ntl_ids_in_many_rows)\n",
    "    & ~df[\"MedicaidSystemID\"].isin(medicaid_ids_in_many_rows)\n",
    "]\n",
    "\n",
    "multi_ntl_df = df[df[\"name_dob_id\"].isin(ntl_ids_in_many_rows)].copy()\n",
    "\n",
    "# Determine the maximum number of matches in a group\n",
    "multi_ntl_df_wmax = append_max_and_count(multi_ntl_df)\n",
    "\n",
    "# View examples\n",
    "# multi_ntl_df_wmax.sort_values(by=\"name_dob_id\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we divide NTL ids into three cases:\n",
    "#   a) There is _not_ a  unique Medicaid ID for the top match value.\n",
    "#      That is, the same ntl-er is matched to multiple medicaid beneficiaries with same score\n",
    "#      We will ship these to DHCF for further evaluation (count_of_match_points > 1)\n",
    "#   b) There is a unique Medicaid ID for the top match value (count_of_match_points == 1) _and_\n",
    "#      that Medicaid ID is the top match for _only one_ NTL id\n",
    "#   c) Case when there is a unique Medicaid ID _but_\n",
    "#      that unique Medicaid ID is the top match for _multiple_ NTL ids\n",
    "\n",
    "\n",
    "# Case (a)\n",
    "one_to_many_df = multi_ntl_df[\n",
    "    (multi_ntl_df[\"match_points\"] == multi_ntl_df[\"max_match_points\"])\n",
    "    & (multi_ntl_df[\"count_of_match_points\"] > 1)\n",
    "]\n",
    "\n",
    "# Start of cases b and c\n",
    "# looks at ntlers who have one top match\n",
    "one_topmatch = multi_ntl_df[\n",
    "    (multi_ntl_df[\"match_points\"] == multi_ntl_df[\"max_match_points\"])\n",
    "    & (multi_ntl_df[\"count_of_match_points\"] == 1)\n",
    "]\n",
    "\n",
    "# Then, separate into Case (b) (ntl-ers whose medicaid top match is only a match for them,\n",
    "# which we call no_confusion_df_init)\n",
    "# and Case (c) (ntl-ers whose medicaid top match is a match for multiple ntl-ers)\n",
    "# for Case (c) there is only one ntl-er who this happens to,\n",
    "# so we manually examine\n",
    "dup_top = one_topmatch.MedicaidSystemID.value_counts()\n",
    "\n",
    "no_confusion_df_init = one_topmatch[\n",
    "    one_topmatch.MedicaidSystemID.isin(dup_top.index[dup_top == 1])\n",
    "]\n",
    "\n",
    "repeated_df = one_topmatch[\n",
    "    one_topmatch.MedicaidSystemID.isin(dup_top.index[dup_top > 1])\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Out of the {len(no_confusion_df_init)} ntlers with one top medicaid match, for \"\n",
    "    f\"{len(repeated_df)} of those, the medicaid benef. was matched to multiple ntl ids\"\n",
    ")\n",
    "\n",
    "## hand examine the single repeated matach\n",
    "# print(repeated_df)\n",
    "\n",
    "## clear match among the two so we just keep one of them\n",
    "repeated_tokeep = repeated_df[\n",
    "    repeated_df.name_dob_id.astype(str).str.contains(\"RICE\")\n",
    "].copy()\n",
    "\n",
    "## rowbind\n",
    "no_confusion_df = pd.concat([no_confusion_df_init, repeated_tokeep])\n",
    "\n",
    "# Each row should have a unique MedicaidSystemID\n",
    "assert len(no_confusion_df) == no_confusion_df[\"MedicaidSystemID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine unique fuzzy matches above with those who were already\n",
    "# unique in the original data set\n",
    "deduped_ntl_df = pd.concat([one_of_both_id_df, no_confusion_df[COMMON_COLS]])\n",
    "\n",
    "assert len(deduped_ntl_df) == deduped_ntl_df[\"MedicaidSystemID\"].nunique()\n",
    "\n",
    "print(f\"After reducing to best matches, we have {len(deduped_ntl_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: One Medicaid id, Many NTL ids\n",
    "\n",
    "Could be true multiple matches, since could be indication of repeat calls (so slightly diff ntl name and dob). We will hand code these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_medicaid_df = df[df[\"MedicaidSystemID\"].isin(medicaid_ids_in_many_rows)]\n",
    "\n",
    "\n",
    "assert not multi_medicaid_df.duplicated().any()\n",
    "\n",
    "# Write out for hand coding\n",
    "multi_medicaid_df.to_csv(MEDICAID_MULTIPLE_NTL_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in hand coded file\n",
    "# two ras coding independently\n",
    "deduped_medicaid_df_r1 = read_file(MEDICAID_MULTIPLE_NTL_FILE_CODED_R1)\n",
    "deduped_medicaid_df_r2 = read_file(MEDICAID_MULTIPLE_NTL_FILE_CODED_R2)\n",
    "\n",
    "## code inter-rater reliability metric\n",
    "compare_keep = pd.DataFrame(\n",
    "    {\n",
    "        \"keep_r1\": deduped_medicaid_df_r1.keep_match,\n",
    "        \"keep_r2\": deduped_medicaid_df_r2.keep_match,\n",
    "    }\n",
    ")\n",
    "\n",
    "compare_keep[\"agree\"] = np.where(\n",
    "    compare_keep.keep_r1 == compare_keep.keep_r2, True, False\n",
    ")\n",
    "irr = np.sum(compare_keep.agree) / compare_keep.shape[0]\n",
    "print(f\"The IRR between RAs hand coding which match to keep was {round(irr, 4)}\")\n",
    "\n",
    "## get indices of ones to keep\n",
    "matches_keep = compare_keep[compare_keep.agree == True].index.tolist()\n",
    "deduped_medicaid_df = deduped_medicaid_df_r1.copy().rename(\n",
    "    columns={\"keep_match\": \"keep_match_init\"}\n",
    ")\n",
    "deduped_medicaid_df[\"agree\"] = np.where(\n",
    "    deduped_medicaid_df.index.isin(matches_keep), True, False\n",
    ")\n",
    "deduped_medicaid_df[\"keep_match\"] = np.where(\n",
    "    (deduped_medicaid_df.keep_match_init == 1) & (deduped_medicaid_df.agree == True),\n",
    "    1,  # code to keep only if both agree on keep\n",
    "    0,\n",
    ")\n",
    "deduped_medicaid_df.drop(columns=[\"keep_match_init\", \"agree\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix types\n",
    "assert deduped_medicaid_df[\"MedicaidSystemID\"].notna().all()\n",
    "assert not deduped_medicaid_df.MedicaidSystemID.str.contains(\".\", regex=False).any()\n",
    "\n",
    "# Make sure types are correct\n",
    "assert deduped_medicaid_df[\"keep_match\"].notna().all()\n",
    "assert deduped_medicaid_df[\"keep_match\"].isin([0, 1]).all()\n",
    "\n",
    "# Keep the indicated one\n",
    "deduped_medicaid_df = deduped_medicaid_df[deduped_medicaid_df.keep_match == 1].copy()\n",
    "\n",
    "\n",
    "# Combine with previous section and perform some sanity checks\n",
    "deduped_df = pd.concat([deduped_ntl_df, deduped_medicaid_df[COMMON_COLS]])\n",
    "\n",
    "# Check: Each NTL id has *one* Medicaid id. This turns out to be FALSE!\n",
    "# assert (deduped_df.groupby('name_dob_id')['MedicaidSystemID'].nunique() == 1).all()\n",
    "\n",
    "# What's left?\n",
    "still_has_dupes = deduped_df[\n",
    "    deduped_df.groupby(\"name_dob_id\")[\"MedicaidSystemID\"].transform(\n",
    "        lambda x: len(np.unique(x))\n",
    "    )\n",
    "    > 1\n",
    "].sort_values(by=\"name_dob_id\")\n",
    "append_max_and_count(still_has_dupes)\n",
    "kept_dupes = still_has_dupes[\n",
    "    still_has_dupes[\"match_points\"] == still_has_dupes[\"max_match_points\"]\n",
    "]\n",
    "\n",
    "# Append to our original deduped_df\n",
    "deduped_df = pd.concat(\n",
    "    [\n",
    "        deduped_df[~deduped_df[\"name_dob_id\"].isin(still_has_dupes[\"name_dob_id\"])],\n",
    "        kept_dupes[COMMON_COLS],\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The final file has {deduped_df.MedicaidSystemID.nunique()} beneficiaries \"\n",
    "    f\"appearing {len(deduped_df)} times\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Merging DHCF responses to (1.1)\n",
    "\n",
    "In this section, we merge back in the results we obtained from DHCF after sending them the output of `1.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge in the file we shared with them that has first and last name and dob\n",
    "dhcf_df = read_file(FILE_SHARED_DHCF)\n",
    "\n",
    "dhcf_df[\"FirstLastName1\"] = dhcf_df.firstname_name1 + \" \" + dhcf_df.lastname_name1\n",
    "# dhcf_df = dhcf_df[['ntl_id', 'phone_number', 'FirstLastName1', 'date_of_birth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## turn dob to string\n",
    "dhcf_df[\"name_dob_id\"] = (\n",
    "    dhcf_df[\"date_of_birth\"].astype(str) + \"_\" + dhcf_df[\"FirstLastName1\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"There are {dhcf_df.ntl_id.nunique()} unique NTL ids in file shared with them to merge\"\n",
    ")\n",
    "print(\n",
    "    f\"There are {dhcf_df.name_dob_id.nunique()} unique names in file shared with them to merge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Cleaning up duplicates from DHCF\n",
    "\n",
    "By examining the data by hand, we remove several sources of duplicate `ntl_id`s. In order:\n",
    "  1. The presence vs non-presence of a `phone_number` seems to drive a lot of dupes. Drop it.\n",
    "  2. Sometimes other identifiers, in particular date of birth, were missing in one source but\n",
    "     not another. So keep the row with the _most_ identifiers present (upto 1)\n",
    "  3. There are a small number of NTL ids that seem to have slightly different DOBs depending\n",
    "     on the source of the birthday. Make a flag and move on for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "after_step_one = dhcf_df.drop(columns=[\"phone_number\"]).drop_duplicates().copy()\n",
    "\n",
    "# Step 2: Our identifiers are ['FirstLastName1', 'date_of_birth']\n",
    "id_cols = [\"FirstLastName1\", \"date_of_birth\"]\n",
    "all_cols = [\"ntl_id\"] + id_cols\n",
    "\n",
    "# NOTE(khw): This is _slightly_ different than the original file but I think what was intended\n",
    "after_step_one[\"num_ids_present\"] = after_step_one[id_cols].notna().sum(axis=1)\n",
    "after_step_one = append_max_and_count(\n",
    "    after_step_one, id_col=\"ntl_id\", value_col=\"num_ids_present\"\n",
    ")\n",
    "after_step_two = (\n",
    "    after_step_one.loc[\n",
    "        after_step_one[\"max_num_ids_present\"] == after_step_one[\"num_ids_present\"]\n",
    "    ]\n",
    "    .drop(\n",
    "        columns=[\"num_ids_present\", \"max_num_ids_present\", \"count_of_num_ids_present\"]\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Step 3\n",
    "after_step_two[\"is_repeated_ntl\"] = (\n",
    "    after_step_two.groupby(\"ntl_id\")[\"ntl_id\"].transform(len) > 1\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"After attempted deduplication, the proportion of duplicated NTL ids \"\n",
    "    f\"is {after_step_two.is_repeated_ntl.mean():0.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Perform DHCF and Original File merger\n",
    "\n",
    "Now that we've cleaned everything up, merge the two sets of deduplicated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_df = pd.merge(\n",
    "    after_step_two, deduped_df, on=\"name_dob_id\", how=\"left\"\n",
    ").drop_duplicates()\n",
    "\n",
    "assert len(decorated_df) == len(after_step_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Deduplicate the Decorated output of (1.3.2)\n",
    "\n",
    "There are _still_ some duplicates after we've performed the work in 1.3.2. In particular, we\n",
    "perform the following steps:\n",
    "  1. Pull out the rows which don't have duplicate `ntl_id`s\n",
    "  2. Of the remaining rows, grab the rows with the most `match_point`s\n",
    "  3. If there's just one row remaining here, we're golden\n",
    "  4. Else, perform a hand review\n",
    "  5. Combine the three outputs above\n",
    "  6. For any remaining NTL ids, deduplicate down to ntl_id and FirstLastName1 and null out other info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_COLS_PLUS = COMMON_COLS + [\"ntl_id\", \"FirstLastName1\"]\n",
    "\n",
    "# Step 1\n",
    "non_dupe_ntl_df = decorated_df[~decorated_df.duplicated(\"ntl_id\", keep=False)]\n",
    "\n",
    "# Step 2\n",
    "dupe_ntl_df = decorated_df[decorated_df.duplicated(\"ntl_id\", keep=False)]\n",
    "dupe_ntl_df = append_max_and_count(dupe_ntl_df, id_col=\"ntl_id\", inplace=False)\n",
    "dupe_ntl_df = dupe_ntl_df[\n",
    "    dupe_ntl_df[\"match_points\"] == dupe_ntl_df[\"max_match_points\"]\n",
    "]\n",
    "\n",
    "# Step 3\n",
    "step_three_df = dupe_ntl_df.loc[\n",
    "    dupe_ntl_df[\"count_of_match_points\"] == 1, COMMON_COLS_PLUS\n",
    "]\n",
    "\n",
    "# Step 4\n",
    "dupe_ntl_df.loc[\n",
    "    dupe_ntl_df[\"count_of_match_points\"] > 1,\n",
    "    [\n",
    "        \"ntl_id\",\n",
    "        \"MedicaidSystemID\",\n",
    "        \"MemberFullName\",\n",
    "        \"FirstLastName1\",\n",
    "        \"MemberDateofBirth\",\n",
    "        \"date_of_birth\",\n",
    "        \"name_dob_id\",\n",
    "    ],\n",
    "].sort_values(by=[\"ntl_id\", \"MedicaidSystemID\"]).to_csv(\n",
    "    NTL_BENEFITS_MUTLIPLE_REVIEW_FILE, index=False\n",
    ")\n",
    "\n",
    "# Read in hand coded file and make sure types are correct\n",
    "step_four_df = read_file(NTL_BENEFITS_MUTLIPLE_REVIEW_FILE_CODED)\n",
    "assert step_four_df[\"keep_match\"].notna().all()\n",
    "assert step_four_df[\"keep_match\"].isin([0, 1]).all()\n",
    "step_four_df = step_four_df[step_four_df[\"keep_match\"] == 1]\n",
    "\n",
    "# Step 5\n",
    "step_five_df = pd.concat([non_dupe_ntl_df, step_three_df, step_four_df])\n",
    "\n",
    "# Step 6\n",
    "step_six_df = decorated_df[\n",
    "    ~decorated_df[\"ntl_id\"].isin(step_five_df[\"ntl_id\"])\n",
    "].drop_duplicates([\"ntl_id\", \"FirstLastName1\"])[[\"ntl_id\", \"FirstLastName1\"]]\n",
    "step_six_df = cross_join(\n",
    "    step_six_df, pd.DataFrame.from_records([], columns=[\"date_of_birth\"] + COMMON_COLS)\n",
    ")\n",
    "step_six_df[\"name_dob_id\"] = (\n",
    "    step_six_df[\"date_of_birth\"].astype(str) + \"_\" + step_six_df[\"FirstLastName1\"]\n",
    ")\n",
    "\n",
    "final_decorated_df = pd.concat([step_five_df, step_six_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decorated_df[\"medicare_dualenrollee\"] = case_when(\n",
    "    final_decorated_df.MedicaidSystemID.isin(medicare_file.MedicaidSystemID),\n",
    "    \"Medicare enrollee\",\n",
    "    final_decorated_df.MedicaidSystemID.isnull(),\n",
    "    \"Unknown (not matched to Medicaid)\",\n",
    "    \"Not medicare enrollee\",\n",
    ")\n",
    "\n",
    "final_decorated_df.medicare_dualenrollee.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Merge participants file with treatment status\n",
    "\n",
    "In this section, we take the output of the previous section, `final_decorated_df`, which consists of ??? and merge it with the treatment statuses we received from FEMS. This consists of\n",
    "\n",
    "0. Reading the data\n",
    "1. Doing some sanity checks\n",
    "2. Merging the data and writing it out for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0: Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most recent ids and create_date variable\n",
    "call_response_df = read_file(CALL_RESPONSE_POST_TX_FILE)\n",
    "\n",
    "# Constructed ids from fuzzy matching\n",
    "constructed_ids_df = read_file(CONSTRUCTED_IDS_FILE)\n",
    "\n",
    "participants_df = pd.merge(\n",
    "    call_response_df,\n",
    "    constructed_ids_df[[\"num_1\", \"constructed_id\"]].drop_duplicates(),\n",
    "    on=\"num_1\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Summarize which identifiers we gave to medicaid\n",
    "\n",
    "Create three flags:\n",
    "\n",
    "* `has_medicaid_name`: Do they have a first name (NOTE(khw): Original says 'either first or last name' but code says different)\n",
    "* `has_medicaid_dob`: Do they have a date of birth\n",
    "* `has_medicaid_phone`: Do they have a phone number\n",
    "\n",
    "cf. slightly different scheme in `02_create_identifiers` which limits to people with a name\n",
    "TODO(khw): Is this a correct description of what was true before?\n",
    "\n",
    "Finally, merge with the above decorated data and write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify main data\n",
    "for target_col, source_col in zip(\n",
    "    [\"name\", \"dob\", \"phone\"], [\"firstname_name1\", \"date_of_birth\", \"phone_number\"]\n",
    "):\n",
    "    participants_df[f\"has_medicaid_{target_col}\"] = participants_df[\"num_1\"].isin(\n",
    "        find_ids(dhcf_df, \"ntl_id\", source_col)\n",
    "    )\n",
    "\n",
    "# updated has no medicaid identifiers\n",
    "# to rely on above indicator flags\n",
    "# rather than dhcf_df in statement\n",
    "participants_df[\"has_no_medicaid_ids\"] = np.where(\n",
    "    (participants_df.has_medicaid_name == False)\n",
    "    & (participants_df.has_medicaid_dob == False)\n",
    "    & (participants_df.has_medicaid_phone == False),\n",
    "    True,\n",
    "    False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_ntl = pd.merge(\n",
    "    participants_df,\n",
    "    final_decorated_df[\n",
    "        [\n",
    "            \"ntl_id\",\n",
    "            \"MedicaidSystemID\",\n",
    "            \"medicare_dualenrollee\",\n",
    "            \"MemberFullName\",\n",
    "            \"MemberDateofBirth\",\n",
    "            \"name_dob_id\",\n",
    "        ]\n",
    "    ],\n",
    "    left_on=\"num_1\",\n",
    "    right_on=\"ntl_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "decorated_ntl[\"has_medicaid_id\"] = case_when(\n",
    "    decorated_ntl[\"MedicaidSystemID\"].isnull(), \"Missing Medicaid ID\", \"Has Medicaid ID\"\n",
    ")\n",
    "\n",
    "## crosstabs of presence\n",
    "pd.crosstab(decorated_ntl.has_medicaid_id, decorated_ntl.dispo_broad)\n",
    "\n",
    "## crosstab of proportions\n",
    "pd.crosstab(\n",
    "    decorated_ntl.has_medicaid_id, decorated_ntl.dispo_broad, normalize=\"columns\"\n",
    ")\n",
    "\n",
    "decorated_ntl[\"id_status\"] = case_when(\n",
    "    ~decorated_ntl.has_medicaid_name & ~decorated_ntl.has_medicaid_dob,\n",
    "    \"Missing dob and name\",\n",
    "    decorated_ntl.has_medicaid_name & ~decorated_ntl.has_medicaid_dob,\n",
    "    \"Missing dob; has name\",\n",
    "    ~decorated_ntl.has_medicaid_name & decorated_ntl.has_medicaid_dob,\n",
    "    \"Missing name; has dob\",\n",
    "    \"Has both\",\n",
    ")\n",
    "\n",
    "# Commented out for pushing to github\n",
    "# ntl_withmedicaid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write new version since updated since round 1 to be able to compare\n",
    "decorated_ntl.to_csv(DECORATED_NTL_OUTPUT_FILE, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
