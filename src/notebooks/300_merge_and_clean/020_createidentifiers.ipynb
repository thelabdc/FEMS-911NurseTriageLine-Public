{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Iterable, Optional, Union\n",
    "\n",
    "import gender_guesser.detector as gender\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from femsntl.datafiles import EXTERNAL_DIR, INTERMEDIATE_DIR, PRIVATE_DATA_DIR\n",
    "from femsntl.utils import (\n",
    "    clean_amr_names,\n",
    "    extract_DOB_fromname,\n",
    "    process_safetypad_names,\n",
    "    standardize_month,\n",
    "    standardize_year,\n",
    ")\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option(\"display.max_columns\", None)  # or 1000\n",
    "pd.set_option(\"display.max_rows\", None)  # or 1000\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "PULL_SQL_EVEN_IF_EXISTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_na_unique(all_rows):\n",
    "\n",
    "    non_na = all_rows.dropna()\n",
    "    return non_na\n",
    "\n",
    "\n",
    "def try_parser(date):\n",
    "    try:\n",
    "        date_raw = parser.parse(date)\n",
    "        date_clean = date_raw.strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        date_clean = None\n",
    "    return date_clean\n",
    "\n",
    "\n",
    "## gets list of ids for each matched pair\n",
    "def find_ids_foramatch(\n",
    "    orig_name, matched_name, id_lookup: pd.DataFrame, colname_originaldf: str\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes in:\n",
    "        - Original name and matched name from fuzzy matching results (cols in data called via apply and lambda)\n",
    "        - Original data with names and ids\n",
    "        - Str with name of the name column in the original data\n",
    "\n",
    "    Returns:\n",
    "        - Dataframe with start of lookup table\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    subset_basedorig = id_lookup.loc[id_lookup[colname_originaldf] == orig_name]\n",
    "    subset_basedmatch = id_lookup.loc[id_lookup[colname_originaldf] == matched_name]\n",
    "    ids_basedorig = subset_basedorig.num_1.tolist()\n",
    "    ids_basedmatch = subset_basedmatch.num_1.tolist()\n",
    "    all_ids = sorted(list(dict.fromkeys(ids_basedorig + ids_basedmatch)))\n",
    "    all_names = sorted([orig_name, matched_name])\n",
    "    all_names_joined = \"; \".join(all_names)\n",
    "    all_ids_unique_joined = \"; \".join(all_ids)\n",
    "    return pd.Series(\n",
    "        [all_names_joined, all_ids_unique_joined], index=[\"all_names\", \"all_ids\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data from previous notebook \n",
    "\n",
    "Left joins of 1) base NTL data with 2) Safety PAD data (script 0) and 3) AMR data (script 1). Doesn't incorporate medicaid ids bc of issues documented in previous script (none of the ids overlap with the analytic window/sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic = pd.read_pickle(INTERMEDIATE_DIR / \"ntl_withsafetypad_withamr.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean names from safety PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## colnames: first_name and last_name\n",
    "## (names from amr denoted with amr prefix)\n",
    "## make name 1) all caps; 2) FIRST_LAST\n",
    "## 3) strip whitespace\n",
    "df_analytic[\"name_safetypad_init\"] = (\n",
    "    df_analytic.first_name.str.upper() + \"_\" + df_analytic.last_name.str.upper()\n",
    ")\n",
    "df_analytic[\"name_safetypad\"] = df_analytic.name_safetypad_init.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version didn;t clip leading spaces\n",
    "name_and_id = (\n",
    "    df_analytic[[\"num_1\", \"name_safetypad\"]]\n",
    "    .dropna(subset=[\"name_safetypad\"])\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "name_and_id[\"obs\"] = (\n",
    "    name_and_id.groupby(\"num_1\")\n",
    "    .transform(\"cumcount\")\n",
    "    .apply(lambda x: f\"safetypad_name_{x+1}\")\n",
    ")\n",
    "name_and_id = name_and_id.pivot(\n",
    "    index=\"num_1\", columns=\"obs\", values=\"name_safetypad\"\n",
    ").reset_index()\n",
    "df_analytic_withnames = pd.merge(df_analytic, name_and_id, on=\"num_1\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clean names from AMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  Explore 3 names fields:\n",
    "\n",
    "1. amr_PatientFName\n",
    "2. amr_PatientLName\n",
    "3. amr_ApplicantsName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"There are {df_analytic_withnames.amr_PatientFName.nunique()} unique first names (but usually contains other info) in AMR data\"\n",
    ")\n",
    "print(\n",
    "    f\"There are {df_analytic_withnames.amr_PatientLName.nunique()} unique last names in AMR data\"\n",
    ")\n",
    "print(\n",
    "    f\"There are {df_analytic_withnames.amr_ApplicantsName.nunique()} unique combined names in AMR data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  Clean firstnames column\n",
    "1. remove DOB\n",
    "2. Split at comma into last name, first name\n",
    "3. Capitalize and paste into form: FIRSTNAME_LASTNAME for similarity with safety pad identifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of non-names appearing in the name field to remove\n",
    "non_names = [\n",
    "    \"UNK\",\n",
    "    \"CALLER\",\n",
    "    \"UNKNOWN\",\n",
    "    \"MEDICAID\",\n",
    "    \"FEMS\",\n",
    "    \"DOB\",\n",
    "    \"YEARS\",\n",
    "    \"OLD\",\n",
    "    \"MEDCAID\",\n",
    "    \"MEDICIAD\",\n",
    "    \"UNINSURED\",\n",
    "    \"REFUSED\",\n",
    "    \"FFS\",\n",
    "    \"BLUE\",\n",
    "    \"CROSS\",\n",
    "    \"SHIELD\",\n",
    "    \"PRIVATE\",\n",
    "    \"INSURANCE\",\n",
    "    \"MEDICARE\",\n",
    "    \"AND\",\n",
    "    \"MD\",\n",
    "    \"CARE\",\n",
    "    \"NUMBER\",\n",
    "    \"FIRST\",\n",
    "    \"NO\",\n",
    "    \"COMMERCIAL\",\n",
    "    \"AMERIGROUP\",\n",
    "    \"VA\",\n",
    "    \"AMRIHEALTH\",\n",
    "    \"KAISER\",\n",
    "    \"DC\",\n",
    "    \"TRUSTED\",\n",
    "    \"PER\",\n",
    "    \"NURSE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic_withnames[\"amr_fname_cleaned\"] = [\n",
    "    clean_amr_names(name, non_names=non_names)\n",
    "    for name in df_analytic_withnames.amr_PatientFName.tolist()\n",
    "]\n",
    "\n",
    "## split into multiple columns based on space delimiter\n",
    "df_amr_splitnames = df_analytic_withnames.amr_fname_cleaned.str.split(\" \", expand=True)\n",
    "df_amr_splitnames.columns = [\n",
    "    \"amr_cleaned_name\" + str(i) for i in np.arange(1, df_amr_splitnames.shape[1] + 1)\n",
    "]\n",
    "\n",
    "## column bind\n",
    "df_analytic_withnames_withcleaned = pd.concat(\n",
    "    [df_analytic_withnames, df_amr_splitnames], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "## create name column formatted\n",
    "## similarly to SafetyPAD data\n",
    "df_analytic_withnames_withcleaned[\"amr_tocompare_wsafetypad\"] = np.where(\n",
    "    df_analytic_withnames_withcleaned.amr_cleaned_name1.notnull(),\n",
    "    df_analytic_withnames_withcleaned.amr_cleaned_name2\n",
    "    + \"_\"\n",
    "    + df_analytic_withnames_withcleaned.amr_cleaned_name1,\n",
    "    np.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clean non-name identifiers\n",
    "\n",
    "Right now cleans CAD ones\n",
    "There are also some safetypad addresses we could add later\n",
    "Purpose of doing this before doing more with names is to pave way for incorporating these in future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Clean address from CAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, clean up address string\n",
    "## via capitalization, removing space\n",
    "## remove extra spaces and add caps\n",
    "cleaning_one = [\n",
    "    re.sub(\n",
    "        \"\\\\.|WASHINGTON$|WASHINGTON DC$\",\n",
    "        \"\",\n",
    "        re.sub(\"\\\\s+\", \" \", str(one_address).upper()),\n",
    "    ).rstrip()\n",
    "    if not pd.isna(one_address)\n",
    "    else \"\"\n",
    "    for one_address in df_analytic_withnames_withcleaned.cstr_add\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic_withnames_withcleaned[\"cleaned_address_CAD\"] = cleaning_one\n",
    "print(\n",
    "    \"Originally there were \"\n",
    "    + str(df_analytic_withnames_withcleaned.cstr_add.nunique())\n",
    "    + \" unique addresses\"\n",
    ")\n",
    "print(\n",
    "    \"Reduced to \"\n",
    "    + str(df_analytic_withnames_withcleaned.cleaned_address_CAD.nunique())\n",
    "    + \" unique addresses after cleaning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Clean phone numbers from CAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_numbers = []\n",
    "for number in df_analytic_withnames_withcleaned.clrnum:\n",
    "    if pd.isna(number):\n",
    "        clean_numbers.append(None)\n",
    "        continue\n",
    "\n",
    "    number = re.sub(\"\\\\-|\\\\s+|\\\\.\", \"\", str(number))\n",
    "    if number in [\"1111111111\", \"NOPHONE\", \"TESTCALL\", \"RADIO\", \"11111111111111\"]:\n",
    "        number = None\n",
    "    clean_numbers.append(number)\n",
    "\n",
    "df_analytic_withnames_withcleaned[\"cleaned_numbers_CAD\"] = clean_numbers\n",
    "\n",
    "print(\n",
    "    \"Before cleaning, there were \"\n",
    "    + str(len(df_analytic_withnames_withcleaned.clrnum.unique()))\n",
    "    + \" unique numbers\"\n",
    ")\n",
    "print(\n",
    "    \"After cleaning, there are \"\n",
    "    + str(len(df_analytic_withnames_withcleaned.cleaned_numbers_CAD.unique()))\n",
    "    + \" unique numbers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3: Clean DOB (in this section, only available for AMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic_withnames_withcleaned[\n",
    "    \"digits_fromfirstname\"\n",
    "] = df_analytic_withnames_withcleaned.amr_PatientFName.apply(extract_DOB_fromname)\n",
    "df_analytic_withnames_withcleaned[\n",
    "    \"digits_fromlastname\"\n",
    "] = df_analytic_withnames_withcleaned.amr_PatientLName.apply(extract_DOB_fromname)\n",
    "\n",
    "# Cascade for birthday: Explicit DOB, then from first name, then last name\n",
    "df_analytic_withnames_withcleaned[\n",
    "    \"amr_dob_updated\"\n",
    "] = df_analytic_withnames_withcleaned.amr_DateofBirth.fillna(\n",
    "    df_analytic_withnames_withcleaned.digits_fromfirstname.fillna(\n",
    "        df_analytic_withnames_withcleaned.digits_fromlastname\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create copy -- use this data later when adding dob to lookup table\n",
    "df_datecleaning = df_analytic_withnames_withcleaned.copy()\n",
    "\n",
    "df_datecleaning[\"dob_backslashdelim\"] = df_datecleaning.amr_dob_updated.str.replace(\n",
    "    \"-\", \"/\"\n",
    ")\n",
    "df_dob_split = df_datecleaning.dob_backslashdelim.str.split(\"\\/\", expand=True)\n",
    "df_dob_split.columns = [\"month\", \"day\", \"year\", \"other\"]\n",
    "\n",
    "##\n",
    "df_dob_split[\"reverse_order\"] = np.where(\n",
    "    df_dob_split.month.str.len() == 4, \"switch_yearandmonth\", \"keep\"\n",
    ")\n",
    "df_dob_split[\"month_toclean\"] = np.where(\n",
    "    df_dob_split.reverse_order == \"switch_yearandmonth\",\n",
    "    df_dob_split.day,\n",
    "    df_dob_split.month,\n",
    ")\n",
    "df_dob_split[\"year_toclean\"] = np.where(\n",
    "    df_dob_split.reverse_order == \"switch_yearandmonth\",\n",
    "    df_dob_split.month,\n",
    "    df_dob_split.year,\n",
    ")\n",
    "df_dob_split[\"day_toclean\"] = np.where(\n",
    "    df_dob_split.reverse_order == \"switch_yearandmonth\",\n",
    "    df_dob_split.year.str.replace(\" 00:00:00\", \"\"),\n",
    "    df_dob_split.day,\n",
    ")\n",
    "\n",
    "\n",
    "month_clean = [standardize_month(str(month)) for month in df_dob_split.month_toclean]\n",
    "df_dob_split[\"month_clean\"] = month_clean\n",
    "year_clean = [standardize_year(str(year)) for year in df_dob_split.year_toclean]\n",
    "df_dob_split[\"year_clean\"] = year_clean\n",
    "df_datecleaning[\"updated_dob_toparse\"] = (\n",
    "    df_dob_split.month_clean\n",
    "    + \"/\"\n",
    "    + df_dob_split.day_toclean\n",
    "    + \"/\"\n",
    "    + df_dob_split.year_clean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dob_ymd = [try_parser(date) for date in df_datecleaning.updated_dob_toparse]\n",
    "df_datecleaning[\"dob_ymd\"] = dob_ymd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Code identifier status for different observations before merging in safetyPAD API data\n",
    "\n",
    "Right now, don't use addresses (since seem much less unique than phone numbers); use phone number and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic_withnames_withcleaned[\"missing_CAD_phone\"] = np.where(\n",
    "    (df_analytic_withnames_withcleaned.cleaned_numbers_CAD.isnull()),\n",
    "    \"Missing phone num.\",\n",
    "    \"Has phone num.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic_withnames_withcleaned[\"missing_safetyPAD_name\"] = np.where(\n",
    "    (df_analytic_withnames_withcleaned.safetypad_name_1.isnull()),\n",
    "    \"Missing safety pad name\",\n",
    "    \"Has safety pad name\",\n",
    ")\n",
    "\n",
    "df_analytic_withnames_withcleaned[\"missing_AMR_name\"] = np.where(\n",
    "    (df_analytic_withnames_withcleaned.amr_tocompare_wsafetypad.isnull()),\n",
    "    \"Missing AMR name\",\n",
    "    \"Has AMR name\",\n",
    ")\n",
    "\n",
    "## code the different categories\n",
    "df_analytic_withnames_withcleaned[\"missing_DOB\"] = np.where(\n",
    "    df_analytic_withnames_withcleaned.amr_dob_updated.isnull(), \"Missing DOB\", \"Has DOB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## redo identifier categories but with date of birth\n",
    "df_analytic_withnames_withcleaned[\"identifier_categories\"] = np.where(\n",
    "    (df_analytic_withnames_withcleaned.missing_DOB == \"Has DOB\")\n",
    "    & (\n",
    "        (\n",
    "            df_analytic_withnames_withcleaned.missing_safetyPAD_name\n",
    "            == \"Has safety pad name\"\n",
    "        )\n",
    "        | (df_analytic_withnames_withcleaned.missing_AMR_name == \"Has AMR name\")\n",
    "    ),\n",
    "    \"Has both name (either amr or safety pad) and DOB\",\n",
    "    np.where(\n",
    "        (df_analytic_withnames_withcleaned.missing_DOB == \"Missing DOB\")\n",
    "        & (\n",
    "            (\n",
    "                df_analytic_withnames_withcleaned.missing_safetyPAD_name\n",
    "                == \"Has safety pad name\"\n",
    "            )\n",
    "            | (df_analytic_withnames_withcleaned.missing_AMR_name == \"Has AMR name\")\n",
    "        ),\n",
    "        \"Has name (either amr or safety pad) but no DOB\",\n",
    "        np.where(\n",
    "            (df_analytic_withnames_withcleaned.missing_DOB == \"Missing DOB.\")\n",
    "            & (\n",
    "                (\n",
    "                    df_analytic_withnames_withcleaned.missing_safetyPAD_name\n",
    "                    == \"Missing safety pad name\"\n",
    "                )\n",
    "                & (\n",
    "                    df_analytic_withnames_withcleaned.missing_AMR_name\n",
    "                    == \"Missing AMR name\"\n",
    "                )\n",
    "            ),\n",
    "            \"Missing name (either) and DOB\",\n",
    "            \"Missing name but has DOB\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "df_analytic_withnames_withcleaned.identifier_categories.value_counts()\n",
    "\n",
    "\n",
    "## before crosstab, find unique\n",
    "df_forcrosstab = df_analytic_withnames_withcleaned.drop_duplicates(\n",
    "    subset=[\"num_1\"], keep=\"first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forcrosstab.loc[\n",
    "    (df_forcrosstab.missing_DOB == \"Missing DOB\")\n",
    "    & (df_forcrosstab.dispo_broad.isin([\"NTL control\", \"NTL treatment\"]))\n",
    "].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. For now, write non-matched data to .csv for ambulance use analysis\n",
    "\n",
    "Ok to use since we're currently only looking at ambulance use for call $c$ rather than for an individual across calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic_withnames_withcleaned.to_csv(INTERMEDIATE_DIR / \"df_forambulanceuse.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Focusing on names, perform fuzzy matching to deduplicate names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 clean data to prepare for fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forfuzzy = df_analytic_withnames_withcleaned.copy()  # copy df so has shorter name\n",
    "\n",
    "## create category for different combinations\n",
    "df_forfuzzy[\"name_status\"] = np.where(\n",
    "    (df_forfuzzy.missing_AMR_name == \"Has AMR name\")\n",
    "    & (df_forfuzzy.missing_safetyPAD_name == \"Has safety pad name\"),\n",
    "    \"Both names\",\n",
    "    np.where(\n",
    "        (df_forfuzzy.missing_AMR_name == \"Missing AMR name\")\n",
    "        & (df_forfuzzy.missing_safetyPAD_name == \"Has safety pad name\"),\n",
    "        \"Safety PAD but not AMR\",\n",
    "        np.where(\n",
    "            (df_forfuzzy.missing_AMR_name == \"Has AMR name\")\n",
    "            & (df_forfuzzy.missing_safetyPAD_name == \"Missing safety pad name\"),\n",
    "            \"AMR not safety PAD\",\n",
    "            \"Neither\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pd.crosstab(df_forfuzzy.name_status, df_forfuzzy.dispo_broad)\n",
    "\n",
    "\n",
    "## for those with both names, check if identical\n",
    "df_forfuzzy[\"identicalname_iftwonames\"] = np.where(\n",
    "    (df_forfuzzy.name_status == \"Both names\")\n",
    "    & (df_forfuzzy.amr_tocompare_wsafetypad == df_forfuzzy.safetypad_name_1),\n",
    "    \"AMR and safety pad same name\",\n",
    "    np.where(\n",
    "        (df_forfuzzy.name_status == \"Both names\")\n",
    "        & (df_forfuzzy.amr_tocompare_wsafetypad != df_forfuzzy.safetypad_name_1),\n",
    "        \"AMR and safety pad diff names\",\n",
    "        \"Doesn't have two names\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "df_forfuzzy.identicalname_iftwonames.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for kevin, sort of a toss up, seems like\n",
    "## both could be useful so converting to long form so that each id is repeated\n",
    "## with the multiple names\n",
    "df_forfuzzy_keycols = df_forfuzzy[\n",
    "    [\"num_1\", \"amr_tocompare_wsafetypad\", \"safetypad_name_1\"]\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make long form\n",
    "## and drop duplicates so that\n",
    "## only ids that have different names in\n",
    "## the two datasets are retained\n",
    "df_forfuzzy_keycols_long = (\n",
    "    df_forfuzzy_keycols.melt(id_vars=[\"num_1\"])\n",
    "    .drop_duplicates(subset=[\"num_1\", \"value\"])\n",
    "    .sort_values(by=\"num_1\")\n",
    ")\n",
    "\n",
    "## removing col that indicates source of name\n",
    "df_forfuzzy_keycols_long_touse = df_forfuzzy_keycols_long[[\"num_1\", \"value\"]]\n",
    "df_forfuzzy_keycols_long_touse.columns = [\"num_1\", \"first_andlast_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forfuzzy_keycols_long_touse[\n",
    "    \"name_nounderscore\"\n",
    "] = df_forfuzzy_keycols_long_touse.first_andlast_name.str.replace(\"_\", \" \")\n",
    "df_forfuzzy_forremote = df_forfuzzy_keycols_long_touse[\n",
    "    [\"num_1\", \"name_nounderscore\"]\n",
    "].copy()\n",
    "\n",
    "df_forfuzzy_forremote.to_csv(INTERMEDIATE_DIR / \"df_forfuzzy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Run fuzzy deduplication\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_threshold = 90\n",
    "output_df_name = INTERMEDIATE_DIR / \"data_withmatches_amrupdates.csv\"\n",
    "\n",
    "## read data\n",
    "data_formatch = df_forfuzzy_forremote.copy()\n",
    "\n",
    "## run\n",
    "all_names = data_formatch.name_nounderscore.unique().tolist()\n",
    "names_tocheck = [\n",
    "    str(name) for name in all_names if name is not None and not pd.isna(name)\n",
    "]\n",
    "\n",
    "\n",
    "def find_fuzzy_namematches(one_name: str) -> pd.DataFrame:\n",
    "\n",
    "    ## choices other than name\n",
    "    other_choices = [choice for choice in names_tocheck if choice != one_name]\n",
    "\n",
    "    ## extract matches above cutoff\n",
    "    all_abovecutoff = process.extractBests(\n",
    "        one_name, other_choices, score_cutoff=match_threshold\n",
    "    )\n",
    "\n",
    "    ## make into a dataframe (will thus only capture ones with matches)\n",
    "    all_abovecutoff_df = pd.DataFrame(\n",
    "        list(all_abovecutoff), columns=[\"matched_name\", \"score\"]\n",
    "    )\n",
    "    all_abovecutoff_df[\"original_name\"] = one_name\n",
    "    return all_abovecutoff_df\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "print(\"Starting fuzzy matching\")\n",
    "t0 = time.time()\n",
    "fuzzymatch_results_list = Parallel(n_jobs=-1, verbose=True)(\n",
    "    delayed(find_fuzzy_namematches)(name) for name in names_tocheck\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"Fuzzy matching took {t1 - t0} seconds to run\")\n",
    "\n",
    "## bind data and write\n",
    "fuzzymatch_results_df = pd.concat(fuzzymatch_results_list)\n",
    "\n",
    "## Sometimes fuzzy matching can lead to a few arbitrary decisions that depend on order\n",
    "## Impose an order\n",
    "fuzzymatch_results_df = (\n",
    "    fuzzymatch_results_df.merge(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"original_name\": names_tocheck,\n",
    "                \"imposed_order\": np.arange(len(names_tocheck), dtype=int),\n",
    "            }\n",
    "        ),\n",
    "        on=\"original_name\",\n",
    "    )\n",
    "    .sort_values(by=\"imposed_order\", kind=\"stable\")\n",
    "    .drop(columns=[\"imposed_order\"])\n",
    ")\n",
    "\n",
    "## write to csv\n",
    "fuzzymatch_results_df.to_csv(output_df_name, index=False)\n",
    "print(\"Script completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3  Read in results and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for now, just matches with other ntl participants\n",
    "matching_results = fuzzymatch_results_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see that it accidentally matched some na's (probably weird thing with writing file)\n",
    "## remove NA's\n",
    "matching_results_complete = matching_results.loc[\n",
    "    (matching_results.matched_name.notnull())\n",
    "    & (matching_results.original_name.notnull())\n",
    "].copy()\n",
    "\n",
    "## most look good, but some are close spelling-wise but different gender\n",
    "matching_results_complete[\n",
    "    \"matched_name_first\"\n",
    "] = matching_results_complete.matched_name.str.replace(\"\\\\s.*\", \"\")\n",
    "matching_results_complete[\n",
    "    \"original_name_first\"\n",
    "] = matching_results_complete.original_name.str.replace(\"\\\\s.*\", \"\")\n",
    "\n",
    "## long-run have RA go through and hand code which\n",
    "## seem like matches versus not (or maybe two and only ones\n",
    "## where they agree)\n",
    "## for now, use heuristic where if one is definitively male,\n",
    "## another definitively female, then they don't count\n",
    "## as matches  (avoiding mark and marika distinction)\n",
    "gen_detect = gender.Detector(case_sensitive=False)\n",
    "matching_results_complete[\n",
    "    \"matched_name_first_gender\"\n",
    "] = matching_results_complete.matched_name_first.apply(\n",
    "    lambda x: gen_detect.get_gender(x)\n",
    ")\n",
    "matching_results_complete[\n",
    "    \"original_name_first_gender\"\n",
    "] = matching_results_complete.original_name_first.apply(\n",
    "    lambda x: gen_detect.get_gender(x)\n",
    ")\n",
    "\n",
    "##\n",
    "matching_results_complete[\"countas_match\"] = np.where(\n",
    "    (matching_results_complete.matched_name_first_gender == \"male\")\n",
    "    & (matching_results_complete.original_name_first_gender == \"female\"),\n",
    "    0,\n",
    "    np.where(\n",
    "        (matching_results_complete.matched_name_first_gender == \"female\")\n",
    "        & (matching_results_complete.original_name_first_gender == \"male\"),\n",
    "        0,\n",
    "        1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "## subset to ones that count as matches and create lookup table\n",
    "matching_results_complete_truematch = matching_results_complete.loc[\n",
    "    matching_results_complete.countas_match == 1\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create lookup table\n",
    "## want, for instance, one row for BD with all of their ids\n",
    "## check set equivalence\n",
    "ids_names = df_forfuzzy_forremote.loc[\n",
    "    df_forfuzzy_forremote.name_nounderscore.notnull(), [\"num_1\", \"name_nounderscore\"]\n",
    "]\n",
    "\n",
    "\n",
    "## matching results to merge\n",
    "matching_results_tomerge = matching_results_complete_truematch[\n",
    "    [\"matched_name\", \"original_name\", \"score\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_wideform = matching_results_tomerge.apply(\n",
    "    lambda x: find_ids_foramatch(\n",
    "        x.original_name, x.matched_name, ids_names, \"name_nounderscore\"\n",
    "    ),\n",
    "    axis=1,\n",
    ").drop_duplicates(keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_wideform_nameexpanded = lookup_wideform.all_names.str.split(\";\", expand=True)\n",
    "lookup_wideform_idsexpanded = lookup_wideform.all_ids.str.split(\";\", expand=True)\n",
    "lookup_wideform_nameexpanded.columns = [\n",
    "    \"name\" + str(i) for i in np.arange(1, lookup_wideform_nameexpanded.shape[1] + 1)\n",
    "]\n",
    "lookup_wideform_idsexpanded.columns = [\n",
    "    \"ntlid_\" + str(i) for i in np.arange(1, lookup_wideform_idsexpanded.shape[1] + 1)\n",
    "]\n",
    "\n",
    "### combine\n",
    "lookup_expanded = pd.concat(\n",
    "    [lookup_wideform_nameexpanded, lookup_wideform_idsexpanded], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Add back respondents who we didnt find matches for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rowbind with others\n",
    "## who either dont have name\n",
    "## or dont have name fuzzy matched\n",
    "lookup_allids = list(\n",
    "    dict.fromkeys(pd.melt(lookup_expanded, id_vars=[\"name1\", \"name2\"]).value.tolist())\n",
    ")\n",
    "print(\"There are \" + str(len(lookup_allids)) + \" ids with fuzzy matched names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forfuzzy_tomerge = (\n",
    "    df_forfuzzy.loc[\n",
    "        ~df_forfuzzy.num_1.isin(lookup_allids),\n",
    "        [\"num_1\", \"amr_tocompare_wsafetypad\", \"safetypad_name_1\"],\n",
    "    ]\n",
    "    .drop_duplicates(keep=\"first\")\n",
    "    .sort_values(by=\"num_1\")\n",
    ")\n",
    "\n",
    "df_forfuzzy_tomerge[\"name1\"] = np.where(\n",
    "    df_forfuzzy_tomerge.safetypad_name_1.notnull(),\n",
    "    df_forfuzzy_tomerge.safetypad_name_1.str.replace(\"_\", \" \"),\n",
    "    np.where(\n",
    "        df_forfuzzy_tomerge.amr_tocompare_wsafetypad.notnull(),\n",
    "        df_forfuzzy_tomerge.amr_tocompare_wsafetypad.str.replace(\"_\", \" \"),\n",
    "        np.nan,\n",
    "    ),\n",
    ")\n",
    "\n",
    "df_forfuzzy_tomerge_final = df_forfuzzy_tomerge[\n",
    "    [\"num_1\", \"name1\"]\n",
    "].copy()  # final to rowbind with ones matched\n",
    "\n",
    "\n",
    "df_forfuzzy_tomerge_final.columns = [\"ntlid_1\", \"name1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Create long-format lookup table with a new id we construct\n",
    "\n",
    "Repeats respondents different ntl ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create new data to rowbind\n",
    "## this data has its original name as name1,\n",
    "## original id as ntlid_1, and then adds other cols\n",
    "## to be empty\n",
    "df_idsnotinlookup_extracols = pd.DataFrame(\n",
    "    data=None, columns=[\"name2\"] + [\"ntlid_\" + str(i) for i in np.arange(2, 14)]\n",
    ")\n",
    "\n",
    "df_idsnotinlookup_tobind_final = pd.concat(\n",
    "    [df_forfuzzy_tomerge_final, df_idsnotinlookup_extracols], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "df_lookup_all = pd.concat([lookup_expanded, df_idsnotinlookup_tobind_final])\n",
    "\n",
    "## create one id per row\n",
    "df_lookup_all_reshuffled = df_lookup_all.sort_values(by=\"ntlid_1\").reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_all_reshuffled[\"constructed_id\"] = df_lookup_all_reshuffled.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_all_forlong = df_lookup_all_reshuffled[\n",
    "    [\n",
    "        col\n",
    "        for col in df_lookup_all_reshuffled.columns\n",
    "        if \"constructed_id\" in col or \"ntlid\" in col\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "## create long format (better for analyses)\n",
    "df_lookup_all_long = pd.melt(\n",
    "    df_lookup_all_forlong, id_vars=\"constructed_id\"\n",
    ").sort_values(by=\"constructed_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_all_long_complete = df_lookup_all_long[\n",
    "    df_lookup_all_long.value.notnull()\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge back with names\n",
    "df_allnames = df_lookup_all_reshuffled[\n",
    "    [\n",
    "        col\n",
    "        for col in df_lookup_all_reshuffled.columns\n",
    "        if \"constructed_id\" in col or \"ntlid\" not in col\n",
    "    ]\n",
    "]\n",
    "df_lookup_all_wnames = pd.merge(\n",
    "    df_lookup_all_long_complete, df_allnames, on=\"constructed_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "df_lookup_all_wnames.rename(\n",
    "    columns={\"variable\": \"which_identifier\", \"value\": \"num_1\"}, inplace=True\n",
    ")\n",
    "\n",
    "df_lookup_all_wnames[\n",
    "    \"num_1\"\n",
    "] = df_lookup_all_wnames.num_1.str.strip()  # whitespace causing ids to appear twice\n",
    "df_lookup_all_wnames_final = df_lookup_all_wnames.drop_duplicates(\n",
    "    subset=\"num_1\", keep=\"first\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Originally, there were \"\n",
    "    + str(len(df_lookup_all_wnames_final.num_1.unique()))\n",
    "    + \" NTL ids\"\n",
    ")\n",
    "print(\n",
    "    \"In current lookup table, there are \"\n",
    "    + str(len(df_lookup_all_wnames_final.constructed_id.unique()))\n",
    "    + \" unique individuals\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Add other attributes to lookup table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Phone numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get ids and phone numbers from original data\n",
    "df_mergephone = (\n",
    "    df_analytic_withnames_withcleaned[[\"num_1\", \"cleaned_numbers_CAD\"]]\n",
    "    .drop_duplicates(keep=\"first\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "## left join onto lookup table\n",
    "df_lookup_all_wnames_wnumbers = pd.merge(\n",
    "    df_lookup_all_wnames_final, df_mergephone, on=\"num_1\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Treatment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## timestamp and treatment status\n",
    "df_mergetxstatus = (\n",
    "    df_analytic[[\"num_1\", \"dispo_broad\", \"event_status\", \"date\"]]\n",
    "    .drop_duplicates(keep=\"first\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "df_lookup_all_wnames_wnumbers_wtx = pd.merge(\n",
    "    df_lookup_all_wnames_wnumbers, df_mergetxstatus, on=\"num_1\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 DOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 DOB based on AMR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mergedob = df_datecleaning[[\"num_1\", \"dob_ymd\"]].drop_duplicates(keep=\"first\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_all_wnames_wnumbers_wtx_wdob = pd.merge(\n",
    "    df_lookup_all_wnames_wnumbers_wtx, df_mergedob, on=\"num_1\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 DOB based on safetyPAD API pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dem_safetypad_comprehensive = pd.read_csv(\n",
    "    PRIVATE_DATA_DIR / \"dem_fromsafetyPAD_20191115.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "extra_dem_safetypad_tomerge = (\n",
    "    extra_dem_safetypad_comprehensive[[\"fems_id\", \"date_of_birth\"]]\n",
    "    .copy()\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "extra_dem_safetypad_tomerge.rename(\n",
    "    columns={\"date_of_birth\": \"date_of_birth_safetypad\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## left join on fems ID\n",
    "df_lookup_all_wnames_wnumbers_wtx_wdob.rename(\n",
    "    columns={\"dob_ymd\": \"date_of_birth_AMR\", \"date\": \"date_call\"}, inplace=True\n",
    ")\n",
    "df_lookup_wAPIdob = pd.merge(\n",
    "    df_lookup_all_wnames_wnumbers_wtx_wdob,\n",
    "    extra_dem_safetypad_tomerge,\n",
    "    left_on=\"num_1\",\n",
    "    right_on=\"fems_id\",\n",
    "    how=\"left\",\n",
    ").drop_duplicates()\n",
    "\n",
    "\n",
    "## create dob final\n",
    "df_lookup_wAPIdob[\"final_dob\"] = np.where(\n",
    "    df_lookup_wAPIdob.date_of_birth_AMR.notnull(),\n",
    "    df_lookup_wAPIdob.date_of_birth_AMR,\n",
    "    np.where(\n",
    "        df_lookup_wAPIdob.date_of_birth_safetypad.notnull(),\n",
    "        df_lookup_wAPIdob.date_of_birth_safetypad,\n",
    "        np.nan,\n",
    "    ),\n",
    ")\n",
    "\n",
    "## code identifier status with dob added\n",
    "df_lookup_wAPIdob[\"id_status\"] = np.where(\n",
    "    (df_lookup_wAPIdob.name1.notnull()) & (df_lookup_wAPIdob.final_dob.notnull()),\n",
    "    \"Name and DOB\",\n",
    "    np.where(\n",
    "        (df_lookup_wAPIdob.name1.notnull()) & (df_lookup_wAPIdob.final_dob.isnull()),\n",
    "        \"Name but no DOB\",\n",
    "        np.where(\n",
    "            (df_lookup_wAPIdob.name1.isnull())\n",
    "            & (df_lookup_wAPIdob.final_dob.isnull())\n",
    "            & (df_lookup_wAPIdob.cleaned_numbers_CAD.notnull()),\n",
    "            \"Only phone number\",\n",
    "            \"None\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "df_idsummary_nondup = df_lookup_wAPIdob.drop_duplicates(\n",
    "    subset=\"num_1\", keep=\"first\"\n",
    ").loc[df_lookup_wAPIdob.dispo_broad.isin([\"NTL control\", \"NTL treatment\"])]\n",
    "print(\n",
    "    pd.crosstab(\n",
    "        df_idsummary_nondup.id_status,\n",
    "        df_idsummary_nondup.dispo_broad,\n",
    "        normalize=\"columns\",\n",
    "    ).to_latex()\n",
    ")\n",
    "print(\n",
    "    pd.crosstab(\n",
    "        df_idsummary_nondup.id_status, df_idsummary_nondup.dispo_broad\n",
    "    ).to_latex()\n",
    ")\n",
    "\n",
    "\n",
    "## write to csv\n",
    "df_lookup_wAPIdob.to_csv(INTERMEDIATE_DIR / \"df_forrepeatcalls.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Create version for DHCR medicaid id lookup\n",
    "\n",
    "(Not changing code so that we can re-create the file we sent them/so that the file is static)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fordhcr = df_lookup_all_wnames_wnumbers_wtx_wdob.copy()\n",
    "\n",
    "## parse names if multiple (could make more efficient with function)\n",
    "df_fordhcr[\"firstname_name1\"] = df_fordhcr[\"name1\"].str.split(\" \").str[0]\n",
    "lastnames_name1 = df_fordhcr[\"name1\"].str.split(\" \").str[1:]\n",
    "lastnames_name1_clean = [\n",
    "    \" \".join(name) if isinstance(name, list) else None for name in lastnames_name1\n",
    "]\n",
    "df_fordhcr[\"lastname_name1\"] = lastnames_name1_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fordhcr[\"firstname_name2\"] = df_fordhcr[\"name2\"].str.strip().str.split(\" \").str[0]\n",
    "lastnames_name2 = df_fordhcr[\"name2\"].str.strip().str.split(\" \").str[1:]\n",
    "lastnames_name2_clean = [\n",
    "    \" \".join(name) if isinstance(name, list) else None for name in lastnames_name2\n",
    "]\n",
    "df_fordhcr[\"lastname_name2\"] = lastnames_name2_clean\n",
    "\n",
    "# The rename back to dob_ymb is for historical consistency\n",
    "df_fordhcr_relcols = df_fordhcr[\n",
    "    [\n",
    "        \"firstname_name1\",\n",
    "        \"lastname_name1\",\n",
    "        \"firstname_name2\",\n",
    "        \"lastname_name2\",\n",
    "        \"date_of_birth_AMR\",\n",
    "        \"cleaned_numbers_CAD\",\n",
    "        \"constructed_id\",\n",
    "        \"which_identifier\",\n",
    "        \"num_1\",\n",
    "        \"dispo_broad\",\n",
    "    ]\n",
    "].rename(columns={\"date_of_birth_AMR\": \"dob_ymd\"})\n",
    "df_fordhcr_relcols[\"id_status\"] = np.where(\n",
    "    (df_fordhcr_relcols.firstname_name1.notnull())\n",
    "    & (df_fordhcr_relcols.dob_ymd.notnull()),\n",
    "    \"Name and DOB\",\n",
    "    np.where(\n",
    "        (df_fordhcr_relcols.firstname_name1.notnull())\n",
    "        & (df_fordhcr_relcols.dob_ymd.isnull()),\n",
    "        \"Name but no DOB\",\n",
    "        np.where(\n",
    "            (df_fordhcr_relcols.firstname_name1.isnull())\n",
    "            & (df_fordhcr_relcols.dob_ymd.isnull())\n",
    "            & (df_fordhcr_relcols.cleaned_numbers_CAD.notnull()),\n",
    "            \"Only phone number\",\n",
    "            \"Other\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "## just write the ones that\n",
    "cols_rename = {\n",
    "    \"dob_ymd\": \"date_of_birth\",\n",
    "    \"cleaned_numbers_CAD\": \"phone_number\",\n",
    "    \"num_1\": \"ntl_id\",\n",
    "}\n",
    "\n",
    "df_fordhcr_relcols.id_status.value_counts(normalize=True)\n",
    "\n",
    "df_fordhcr_relcols.rename(columns=cols_rename, inplace=True)\n",
    "df_fordhcr_relrows = df_fordhcr_relcols.loc[\n",
    "    df_fordhcr_relcols.id_status.isin([\"Name and DOB\", \"Name but no DOB\"])\n",
    "].copy()\n",
    "df_fordhcr_relrows.to_csv(EXTERNAL_DIR / \"identifiers_fordhcr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 subset to those missing DOB and try to add based on safetypad api info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset to those missing dob\n",
    "missing_dob_init = df_fordhcr_relrows.loc[\n",
    "    df_fordhcr_relrows.id_status == \"Name but no DOB\"\n",
    "].copy()\n",
    "\n",
    "\n",
    "extra_dem_safetypad = pd.read_csv(PRIVATE_DATA_DIR / \"dem_fromsafetyPAD.csv\")\n",
    "\n",
    "## capitalize fname and lname and construct key\n",
    "extra_dem_safetypad[\"name_key\"] = (\n",
    "    extra_dem_safetypad.last_name.str.upper()\n",
    "    + \"_\"\n",
    "    + extra_dem_safetypad.first_name.str.upper()\n",
    ")\n",
    "missing_dob_init[\"name_key\"] = (\n",
    "    missing_dob_init.lastname_name1.str.upper()\n",
    "    + \"_\"\n",
    "    + missing_dob_init.firstname_name1.str.upper()\n",
    ")\n",
    "\n",
    "\n",
    "## look for intersecting name keys\n",
    "name_keys_match = set(missing_dob_init.name_key).intersection(\n",
    "    extra_dem_safetypad.name_key\n",
    ")\n",
    "\n",
    "## check how many name keys match exactly\n",
    "print(\n",
    "    \"There are \"\n",
    "    + str(len(name_keys_match))\n",
    "    + \" name keys that match exactly between base data missing dobs and api pull\"\n",
    ")\n",
    "\n",
    "## using exact name keys, replace DOB with one from API\n",
    "missing_dob_toadd = missing_dob_init.drop(columns=\"date_of_birth\", inplace=False)\n",
    "\n",
    "\n",
    "## merge in dob\n",
    "dob_tomerge = extra_dem_safetypad[[\"name_key\", \"date_of_birth\"]].copy()\n",
    "\n",
    "## left join onto main data\n",
    "missing_dob_toadd_withdob = pd.merge(\n",
    "    missing_dob_toadd, dob_tomerge, on=\"name_key\", how=\"left\"\n",
    ")\n",
    "\n",
    "## drop name key and update id status\n",
    "missing_dob_tomerge = missing_dob_toadd_withdob.drop(columns=\"name_key\", inplace=False)\n",
    "missing_dob_tomerge[\"id_status\"] = np.where(\n",
    "    (missing_dob_tomerge.firstname_name1.notnull())\n",
    "    & (missing_dob_tomerge.date_of_birth.notnull()),\n",
    "    \"Name and DOB\",\n",
    "    \"Name but no DOB\",\n",
    ")\n",
    "\n",
    "\n",
    "## rowbind back\n",
    "missing_dob_tomerge_nodup = missing_dob_tomerge.drop_duplicates()\n",
    "observed_dob_tomerge = df_fordhcr_relrows.loc[\n",
    "    df_fordhcr_relrows.id_status != \"Name but no DOB\"\n",
    "].copy()\n",
    "\n",
    "## combine\n",
    "df_fordhcr_updateddob = pd.concat(\n",
    "    [missing_dob_tomerge_nodup, observed_dob_tomerge]\n",
    ").drop_duplicates()\n",
    "\n",
    "\n",
    "df_fordhcr_relrows.id_status.value_counts(normalize=True)\n",
    "df_fordhcr_updateddob.id_status.value_counts(normalize=True)\n",
    "\n",
    "\n",
    "## save\n",
    "df_fordhcr_updateddob.to_csv(EXTERNAL_DIR / \"df_fordhcr_DOBsadded.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5a16a1a001cdfc4c731771dfa7e8db89dd071e870ad0b0dad72a5f6155243af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
