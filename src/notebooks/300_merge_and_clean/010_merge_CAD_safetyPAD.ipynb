{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "import yaml\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from femsntl.datafiles import (\n",
    "    CREDENTIALS_FILE,\n",
    "    EXTERNAL_DIR,\n",
    "    INTERMEDIATE_DIR,\n",
    "    OUTPUT_DIR,\n",
    "    PKL_FILE,\n",
    "    PRIVATE_DATA_DIR,\n",
    "    SAFETYPAD_DIR,\n",
    "    SQL_DUMP_FILE,\n",
    ")\n",
    "from femsntl.themes import *\n",
    "from femsntl.utils import clean_column_names, longform_crosstab\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option(\"display.max_columns\", None)  # or 1000\n",
    "pd.set_option(\"display.max_rows\", None)  # or 1000\n",
    "pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTL_START_DATE = \"2018-03-19\"\n",
    "NTL_END_DATE = \"2019-03-01\"\n",
    "\n",
    "PULL_SQL_EVEN_IF_EXISTS = False\n",
    "PULL_DBFILE_FROM_PKL = True\n",
    "EXPORT_TO_SAFETYPAD = False\n",
    "STORE_AS_PARQUET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "\n",
    "Two versions:\n",
    "\n",
    "- Data pulled from raw query similar to Nicole's: contains updated info we want like phone number and address; drawback is that it doesn't have the long ifelse chain that overrides the original disposition codes for certain IDs\n",
    "- Data pulled from the Tableau data she sent in an email: contains the correct disposition codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "if not PULL_DBFILE_FROM_PKL:\n",
    "    with open(CREDENTIALS_FILE, \"rt\") as cred_file:\n",
    "        creds = yaml.load(cred_file)\n",
    "\n",
    "    # Setup connection\n",
    "    conn = pymssql.connect(**creds[\"cad_db\"])\n",
    "\n",
    "    # query text\n",
    "    # CAD Query that loads our data\n",
    "    ntl_summary_query = \"\"\"\n",
    "    SELECT\n",
    "      ae.eid,\n",
    "      ae.num_1,\n",
    "      ae.sdts,\n",
    "      ae.dgroup,\n",
    "      ae.tycod,\n",
    "      ae.typ_eng,\n",
    "      ae.xdts,\n",
    "      ae.ecbd_id,\n",
    "      ae.status_code AS status_code, \n",
    "      ae.xcmt AS ae_xcmt, \n",
    "      ae.ssec,\n",
    "      ae.ad_sec,\n",
    "      ntl.cdts,\n",
    "      ae.ds_ts,\n",
    "      ce.edirpre,\n",
    "      ce.estnum,\n",
    "      ce.efeanme,\n",
    "      ce.eapt, \n",
    "      ce.efeatyp,\n",
    "      ce.edirsuf,\n",
    "      ce.loc_com, \n",
    "      ce.ecompl,\n",
    "      ntl.num_1 AS ntl_num_1,\n",
    "      ntl.external_event_id,\n",
    "      ntl.dispo,\n",
    "      ntl.xcmt AS ntl_xcmt,\n",
    "      ec.comm,\n",
    "      cec.clname,\n",
    "      cec.clrnum,\n",
    "      cec.cstr_add\n",
    "    FROM AGENCY_EVENT ae\n",
    "    LEFT JOIN common_event_call cec ON ae.eid = cec.eid \n",
    "    LEFT JOIN common_event ce ON ae.eid = ce.eid \n",
    "    LEFT JOIN ntl_cache ntl ON ae.num_1 = ntl.num_1\n",
    "    LEFT JOIN EVCOM ec ON\n",
    "      ae.eid = ec.eid AND\n",
    "      ec.COMM = 'NTL ** CANCEL REQUESTED BY ECBD. TRANSFER EVENT'\n",
    "    WHERE\n",
    "      LEFT(ae.sdts, 12) >= '201804190900' AND \n",
    "      (ntl.num_1 IS NOT NULL OR (ae.TYCOD LIKE '%NTL%' AND LEFT(ae.sdts, 8) < '20180531')) AND\n",
    "      ISNULL(LEFT(ae.XCMT, 4), 'T') <> 'TEST' AND\n",
    "      ISNULL(ae.XCMT, 'T') <> 'CBD TEST'\n",
    "    \"\"\"\n",
    "\n",
    "    ntl_summary_raw = pd.read_sql_query(ntl_summary_query, conn)\n",
    "    ntl_summary_raw.to_parquet(SQL_DUMP_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below uses Nicole's query with two additions: \n",
    "- Adding in the left join to common_event_call which gives us names and phone numbers\n",
    "- Adding in the left join to common_event which gives us more details on addresses and also the name of commonly-recognized locations (ecompl)\n",
    "\n",
    "In future, could restrict date range so that it doesn't load more recent events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to read in data from pkl or parquet backup rather than\n",
    "# directly from DB\n",
    "if SQL_DUMP_FILE.exists() and not PULL_SQL_EVEN_IF_EXISTS:\n",
    "    ntl_summary_raw = pd.read_parquet(SQL_DUMP_FILE)\n",
    "elif PULL_DBFILE_FROM_PKL:\n",
    "    ntl_summary_raw = pd.read_pickle(PKL_FILE)\n",
    "\n",
    "\n",
    "print(f\"Number of rows in raw query {len(ntl_summary_raw)}\")\n",
    "\n",
    "# Pull in static file from Nicole\n",
    "ntl_summary_clean = pd.read_csv(PRIVATE_DATA_DIR / \"ntl_data_tableau.csv\")\n",
    "print(f\"Number of rows in Nicole's tableau table is {len(ntl_summary_clean)}\")\n",
    "\n",
    "## difference reflects i think exclusions for testing/ineligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix column names for cleaned data\n",
    "ntl_summary_clean.columns = clean_column_names(ntl_summary_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge on the basis of agency_event with the data from the raw query\n",
    "## this preserves the info from the raw database but includes the tableau-coded\n",
    "## overriden event codes\n",
    "ntl_analytic = ntl_summary_raw.merge(\n",
    "    ntl_summary_clean[\n",
    "        [\"agency_event\", \"reported_event_status\", \"event_status\", \"ntl_id\"]\n",
    "    ],\n",
    "    left_on=\"num_1\",\n",
    "    right_on=\"agency_event\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial data cleaning and merging in Safety PAD\n",
    "\n",
    "- Subset observations to dates of study\n",
    "- Create informative names for disposition codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pull date and make date-time object\n",
    "ntl_analytic[\"date\"] = pd.to_datetime(\n",
    "    ntl_analytic.sdts.str.extract(\"(?P<date_str>201[8|9]\\d{10})\").date_str,\n",
    "    format=\"%Y%m%d%H%M%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset to calls within the evaluation period\n",
    "ntl_summary_eval = ntl_analytic[\n",
    "    (ntl_analytic.date >= NTL_START_DATE) & (ntl_analytic.date < NTL_END_DATE)\n",
    "].copy()\n",
    "\n",
    "print(f\"Rows in raw data: {len(ntl_analytic)}\")\n",
    "print(f\"Rows in NTL range: {len(ntl_summary_eval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create variable just truncated to the month level\n",
    "ntl_summary_eval[\"month_year\"] = ntl_summary_eval.date.dt.to_period(\"M\")\n",
    "ntl_summary_eval[\"month_year_day\"] = ntl_summary_eval.date.dt.to_period(\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Descriptive one: number of observations in different study groups over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count of disposition code by month\n",
    "dispo_bymonth = pd.crosstab(ntl_summary_eval.event_status, ntl_summary_eval.month_year)\n",
    "all_event_codes = ntl_summary_eval.event_status.unique().tolist()\n",
    "\n",
    "ntl_handled_codes = [\n",
    "    code for code in all_event_codes if \"NTL Handled\" in code or \"NTL - Other\" in code\n",
    "]\n",
    "field_requested_codes = [\n",
    "    code for code in all_event_codes if \"Field Requested NTL\" in code\n",
    "]\n",
    "transfer_codes = [code for code in all_event_codes if \"Transfer from NTL\" in code]\n",
    "control_codes = [\"Study Reject\"]\n",
    "out_of_band_codes = [\"Request outside the hours of operation\"]\n",
    "other_treatment_codes = [\n",
    "    \"Incompatible Code from AMR\",\n",
    "    \"RESP EXPIRED\",\n",
    "    \"SERVICE NOT AVAILABLE\",\n",
    "]  # new tx codes as indicated by nicole\n",
    "treatment_codes = (\n",
    "    ntl_handled_codes + field_requested_codes + transfer_codes + other_treatment_codes\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"We are assuming that the treatment group event codes are:\\n  * \"\n",
    "    + \"\\n  * \".join(treatment_codes)\n",
    ")\n",
    "\n",
    "other_codes = [\n",
    "    code\n",
    "    for code in all_event_codes\n",
    "    if code not in (control_codes + treatment_codes + out_of_band_codes)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize broad categories\n",
    "ntl_summary_eval[\"dispo_broad\"] = \"Other\"\n",
    "for label, group in [\n",
    "    (\"NTL treatment\", treatment_codes),\n",
    "    (\"NTL control\", control_codes),\n",
    "    (\"Outside NTL hours\", out_of_band_codes),\n",
    "]:\n",
    "    ntl_summary_eval.loc[\n",
    "        ntl_summary_eval.event_status.isin(group), \"dispo_broad\"\n",
    "    ] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize braod categories by month\n",
    "dispobroad_bymonth = pd.crosstab(\n",
    "    ntl_summary_eval.dispo_broad, ntl_summary_eval.month_year\n",
    ")\n",
    "dispobroad_bymonth_long = longform_crosstab(dispobroad_bymonth, grouping_var=\"dispo\")\n",
    "dispo_txcont = dispobroad_bymonth_long.loc[\n",
    "    dispobroad_bymonth_long.dispo.isin([\"NTL control\", \"NTL treatment\"])\n",
    "].copy()\n",
    "overall_trends = (\n",
    "    ggplot(dispo_txcont, aes(x=\"date\", y=\"value\", color=\"factor(dispo)\"))\n",
    "    + geom_point()\n",
    "    + geom_line()\n",
    "    + standard_background\n",
    "    + theme(\n",
    "        axis_text_x=element_text(angle=90),\n",
    "        legend_position=(0.5, 0.7),\n",
    "        legend_background=element_blank(),\n",
    "    )\n",
    "    + labs(color=\"Broad Group\")\n",
    "    + ylab(\"Count of calls by month\")\n",
    "    + xlab(\"Month and year\")\n",
    "    + scale_color_manual(values=(CONTROL_COLOR, TREATMENT_COLOR))\n",
    ")\n",
    "\n",
    "ggsave(\n",
    "    plot=overall_trends,\n",
    "    filename=OUTPUT_DIR / \"overall_trends_txcont.png\",\n",
    "    dpi=300,\n",
    "    verbose=False,\n",
    ")\n",
    "overall_trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Get IDS of NTL participants to search for them via safetypad interface and export their data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_groups = [\"NTL control\", \"NTL treatment\"]\n",
    "ntl_participants = ntl_summary_eval[\n",
    "    ntl_summary_eval.dispo_broad.isin(study_groups)\n",
    "].copy()\n",
    "n_total = len(ntl_participants.num_1.unique())\n",
    "n_tx = len(\n",
    "    ntl_participants.num_1[ntl_participants.dispo_broad == \"NTL treatment\"].unique()\n",
    ")\n",
    "n_control = len(\n",
    "    ntl_participants.num_1[ntl_participants.dispo_broad == \"NTL control\"].unique()\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Total NTL participants (unique calls; not unique people):\", n_total)\n",
    "print(\"Total treatment:\", n_tx)\n",
    "print(\"Total control:\", n_control)\n",
    "print(f\"Percent is treatment: {n_tx / n_total * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## newly added- get ids of those referred to ntl (but ended up outside of caller hours;\n",
    "## useful for repeat calls analysis because we count as a repeat call if they:\n",
    "##   1) have an ntl code, but\n",
    "##   2) call outside the study window\n",
    "ntl_nonparticipants = ntl_summary_eval[\n",
    "    ~ntl_summary_eval.num_1.isin(ntl_participants.num_1)\n",
    "]\n",
    "print(\n",
    "    \"Number of incidents NTL eligible but not randomized:\",\n",
    "    len(ntl_nonparticipants.num_1.unique()),\n",
    ")\n",
    "print(\"e.g. because outside study hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Get IDs in batches of 1000 to export to safety pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.1: ids for treatment and control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_SAFETYPAD == True:\n",
    "\n",
    "    unique_ids = ntl_participants.num_1.unique()\n",
    "\n",
    "    print(\"Unique NTL ids:\", len(unique_ids))\n",
    "    print(\"Total calls in treatment and control:\", n_total)\n",
    "\n",
    "    ## test with first 1000\n",
    "    ## site: https://dcfems.safetypad.com/index.php\n",
    "    ## formatting: no quotes, separated by comma\n",
    "    store_id_batches = [\n",
    "        \", \".join(unique_ids[left : left + 1000])\n",
    "        for left in range(0, len(unique_ids), 1000)\n",
    "    ]\n",
    "\n",
    "    unique_ids_nonparticipants = ntl_nonparticipants.num_1.unique()\n",
    "\n",
    "    print(\"Unique NTL ids in nonparticipant calls:\", len(unique_ids_nonparticipants))\n",
    "    print(\"Total nonparticipant calls:\", len(ntl_nonparticipants))\n",
    "\n",
    "    nonparticipant_store_id_batches = [\n",
    "        \", \".join(unique_ids_nonparticipants[left : left + 1000])\n",
    "        for left in range(0, len(unique_ids_nonparticipants), 1000)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous step of searching ids in safety padresults in 6 files with following naming convention\n",
    "## safetypad_idsearch_batch*.csv (after xls conversion bc struggled with corrupted file format)\n",
    "\n",
    "## concatenate the CSVs\n",
    "safetypad_files = list(SAFETYPAD_DIR.glob(\"safetypad_idsearch_batch*.csv\"))\n",
    "\n",
    "all_safetypad_df = pd.concat([pd.read_csv(filename) for filename in safetypad_files])\n",
    "all_safetypad_df.columns = clean_column_names(all_safetypad_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 For ones who are clinical referral, self care, or others, write list of ids for AMR to search for \n",
    "\n",
    "Note: just included the ones in tx or control group; not other ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create indicator in main data\n",
    "ntl_participants[\"is_in_safetypad\"] = ntl_participants.num_1.isin(\n",
    "    all_safetypad_df.incident_number\n",
    ")\n",
    "\n",
    "## summary\n",
    "amr_statuses = [\"NTL Handled - RSC\", \"NTL Handled - Clinical Referral\", \"NTL - Other\"]\n",
    "print(\n",
    "    \"Number study participants in safety pad:\",\n",
    "    ntl_participants[ntl_participants.is_in_safetypad].num_1.nunique(),\n",
    ")\n",
    "print(\n",
    "    \"Number study participants *not* in safety pad safety pad:\",\n",
    "    ntl_participants[~ntl_participants.is_in_safetypad].num_1.nunique(),\n",
    ")\n",
    "\n",
    "## disposition codes of those not in safety pad\n",
    "ntl_participants_notinsafetypad = ntl_participants[~ntl_participants.is_in_safetypad]\n",
    "\n",
    "## get ids of ones who are RSC, clin. referral, or other\n",
    "ntl_participants_notinsafetypad_checkwithAMR = ntl_participants_notinsafetypad[\n",
    "    ntl_participants_notinsafetypad.event_status.isin(amr_statuses)\n",
    "]\n",
    "\n",
    "## about 1000 participants, write their name and dates to csv file\n",
    "ntl_participants_forAMR = ntl_participants_notinsafetypad_checkwithAMR[\n",
    "    [\"num_1\", \"date\", \"event_status\"]\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Asking AMR to look for {ntl_participants_forAMR.num_1.nunique()}\"\n",
    "    \" non-safety PAD ids who are in self-care, clin referral, or other\"\n",
    ")\n",
    "\n",
    "## AMR participants\n",
    "ntl_participants_forAMR.to_csv(\n",
    "    EXTERNAL_DIR / \"ntl_participants_forAMR.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3: for ones who are other statuses and not cancelled, write ids for Nicole or FEMs to search for\n",
    "\n",
    "Note: included the \"other\" category in both IDs sent to AMR and ids to send to usc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_statuses_toask_OUC = [\n",
    "    col\n",
    "    for col in ntl_participants_notinsafetypad.event_status.unique()\n",
    "    if col\n",
    "    not in [\n",
    "        \"NTL Handled - RSC\",\n",
    "        \"NTL Handled - Clinical Referral\",\n",
    "        \"NTL - Other\",\n",
    "        \"NTL Handled - Canceled\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "ntl_participants_notinsafetypad_checkwithOUC = ntl_participants_notinsafetypad[\n",
    "    ntl_participants_notinsafetypad.event_status.isin(event_statuses_toask_OUC)\n",
    "]\n",
    "\n",
    "ntl_participants_forOUC = ntl_participants_notinsafetypad_checkwithOUC[\n",
    "    [\"num_1\", \"date\", \"event_status\"]\n",
    "]\n",
    "\n",
    "## OUC participants\n",
    "ntl_participants_forOUC.to_csv(\n",
    "    EXTERNAL_DIR / \"ntl_participants_forOUC.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4: do a left join between 1) CAD and 2) safety PAD for the purposes of repeat calls analysis\n",
    "\n",
    "This means we'll retain all participants, but 1300 or so will be missing safetyPAD info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## also read in safetypad data on non participants\n",
    "## since that could count as a repeat call for a participant\n",
    "safetypad_nonparticipants = pd.read_csv(\n",
    "    SAFETYPAD_DIR / \"safetypad_idsearch_nonparticipants.csv\"\n",
    ")\n",
    "safetypad_nonparticipants.columns = clean_column_names(\n",
    "    safetypad_nonparticipants.columns\n",
    ")\n",
    "\n",
    "all_safetypad_df_withnon = pd.concat([all_safetypad_df, safetypad_nonparticipants])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## left join safety pad to all ntl (so includes participants (tx and control) and those who called but werent in tx or control)\n",
    "ntl_withsafetypad = ntl_summary_eval.merge(\n",
    "    all_safetypad_df_withnon, left_on=\"num_1\", right_on=\"incident_number\", how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Number of incidents left join:\", len(ntl_participants))\n",
    "print(\n",
    "    \"Number of incidents after left join (larger since multiple rows for each status):\",\n",
    "    len(ntl_withsafetypad),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean types before dumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps before dumping\n",
    "for col in [\"sdts\", \"XDTS\", \"cdts\", \"ds_ts\"]:\n",
    "    ntl_withsafetypad[col] = pd.to_datetime(\n",
    "        ntl_withsafetypad[col].map(lambda x: x[:-2], na_action=\"ignore\"),\n",
    "        format=\"%Y%m%d%H%M%S\",\n",
    "    ).dt.tz_localize(\"US/Eastern\", ambiguous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\n",
    "    \"en_route\",\n",
    "    \"at_scene\",\n",
    "    \"at_patient\",\n",
    "    \"depart_scene\",\n",
    "    \"at_destination\",\n",
    "    \"in_service\",\n",
    "]:\n",
    "    ntl_withsafetypad[col] = pd.to_datetime(\n",
    "        ntl_withsafetypad[col], format=\"%m/%d/%Y %H:%M\"\n",
    "    ).dt.tz_localize(\"US/Eastern\", ambiguous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet can't handle periods\n",
    "for col in [\"month_year\", \"month_year_day\"]:\n",
    "    ntl_withsafetypad[col] = ntl_withsafetypad[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixup zip codes\n",
    "ntl_withsafetypad[\"zip_code\"] = ntl_withsafetypad.zip_code.map(\n",
    "    lambda x: x if isinstance(x, int) else int(x[:5]), na_action=\"ignore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to parquet or pkl\n",
    "if STORE_AS_PARQUET:\n",
    "    ntl_withsafetypad.to_parquet(INTERMEDIATE_DIR / \"ntl_withsafetypad.parquet\")\n",
    "else:\n",
    "    ntl_withsafetypad.to_pickle(INTERMEDIATE_DIR / \"ntl_withsafetypad.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
